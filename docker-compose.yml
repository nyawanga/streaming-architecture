
services:
  clickhouse-server:
    image: clickhouse/clickhouse-server:24.3.2.23
    container_name: clickhouse-server
    restart: "unless_stopped"
    env_file:
      - .env
    ports:
      - 8123:8123
      - 9000:9000
    networks:
      - franco
    volumes:
      - ./data/clickhouse/db_data:/var/lib/clickhouse/ #- main folder where ClickHouse stores the data
      - ./data/clickhouse/logs:/var/log/clickhouse-server/ #- logs
      - ./data/clickhouse/init:/docker-entrypoint-initdb.d/ # entrypoint
    ulimits:
      nproc: 65535
      nofile:
        soft: 262144
        hard: 262144

  postgres:
    # image: postgres:13
    # build:
    #   context: .
    #   dockerfile: ./Dockerfile.connect
    image: franco-postgres
    container_name: postgres
    ports:
      - 5433:5432
    networks:
      - franco
    volumes:
      - ./data/postgres/db_data/:/var/lib/postgresql/data

    healthcheck:
      test: ["CMD", "pg_isready", "-U", "franco"]
      interval: 5s
      retries: 5
    env_file:
      - .env

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    env_file:
    - .env
    ports:
      - "89:80"
    restart: unless-stopped
    depends_on:
      - postgres
    networks:
      - franco

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper:2888:3888
    networks:
      - franco

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9999:9999"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zookeeper
    networks:
      - franco

  kafka2:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka2
    container_name: kafka2
    ports:
      - "9093:9092"
      - "29093:29092"
      - "10000:9999"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093,DOCKER://host.docker.internal:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_BROKER_ID: 2
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 10000
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zookeeper
    networks:
      - franco

  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop
    restart: "no"
    ports:
      - 9001:9000
    networks:
      - franco
    environment:
      KAFKA_BROKERCONNECT: "kafka:19092,kafka:19093"
      # JVM_OPTS: "-Xms16M -Xmx512M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"
    depends_on:
      - kafka
      - kafka2

  kafka-connect:
    # build:
    #   context: .
    #   dockerfile: ./Dockerfile.connect
    image: franco-connect
    # image: confluentinc/cp-kafka-connect-base:latest
    container_name: kafka-connect
    
    ports:
      - "8082:8083"
    networks:
      - franco
    environment:
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:19092,kafka:19093'
      CONNECT_REST_ADVERTISED_HOST_NAME: 'kafka-connect'
      CONNECT_GROUP_ID: 1
      CONNECT_CONFIG_STORAGE_TOPIC: debezium_configs
      CONNECT_OFFSET_STORAGE_TOPIC: debezium_offsets
      CONNECT_STATUS_STORAGE_TOPIC: debezium_statuses
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"

    depends_on:
      - kafka 
      - kafka2
 

  superset:
    container_name: superset
    build:
      context: .
      dockerfile: ./infra/superset/Dockerfile.superset
    env_file:
      - .env
    ports:
      - "8089:8088"
    networks:
      - franco
    volumes:
      - ./data/superset/home:/home/superset
      - ./data/superset/data:/var/lib/superset
    logging:
      options:
        max-size: 10m
        max-file: "3"

networks:
  franco:
    external: true

